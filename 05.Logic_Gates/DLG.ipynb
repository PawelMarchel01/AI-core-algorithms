{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Classification on the MONK Dataset Using Differentiable Logic Gates\n",
        "\n",
        "This notebook focuses on solving a binary classification task using the MONK dataset. The dataset contains examples described by a set of categorical features, and the goal is to predict whether each instance belongs to the target class.\n",
        "\n",
        "What makes this approach unique is the use of a custom neural network architecture built from differentiable logic gates instead of traditional activation functions. This setup allows us to explore the interpretability and logic-like behavior of learned decision boundaries while still benefiting from gradient-based learning methods.\n"
      ],
      "metadata": {
        "id": "klL1ElC_QbMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "P_yC2GJwAB_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_monk_data(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "    data = data.drop(columns=data.columns[0])\n",
        "\n",
        "    X = data.iloc[:, 1:].values\n",
        "    y = data.iloc[:, 0].values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=2137)"
      ],
      "metadata": {
        "id": "Rerai_dlDDCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differentiable Logic Gate Model\n",
        "\n",
        "The core of the model is a neural network where each layer consists of neurons that behave like logic gates. These gates are designed in a differentiable way, enabling training through backpropagation.\n",
        "\n",
        "The network is configured with:\n",
        "\n",
        "- Input dimension matching the MONK dataset features\n",
        "- Six hidden layers, each containing 24 logic-based neurons\n",
        "- One output neuron for binary classification\n",
        "\n"
      ],
      "metadata": {
        "id": "5xqD0EtiQ9_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DifferentiableLogicGate(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(DifferentiableLogicGate, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.out_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.logic_weights = nn.ParameterList([\n",
        "            nn.Parameter(torch.randn(16)) for _ in range(hidden_dim)\n",
        "        ])\n",
        "\n",
        "    def forward_logic_gate(self, a, b, gate_id):\n",
        "        if gate_id == 0:  # FALSE\n",
        "            return torch.zeros_like(a)\n",
        "        elif gate_id == 1:  # AND\n",
        "            return a * b\n",
        "        elif gate_id == 2:  # NOT(A->B)\n",
        "            return a - a * b\n",
        "        elif gate_id == 3:  # A\n",
        "            return a\n",
        "        elif gate_id == 4:  # NOT(B->A)\n",
        "            return b - a * b\n",
        "        elif gate_id == 5:  # B\n",
        "            return b\n",
        "        elif gate_id == 6:  # XOR\n",
        "            return a + b - 2 * a * b\n",
        "        elif gate_id == 7:  # OR\n",
        "            return a + b - a * b\n",
        "        elif gate_id == 8:  # NOR\n",
        "            return 1 - (a + b - a * b)\n",
        "        elif gate_id == 9:  # XNOR\n",
        "            return 1 - (a + b - 2 * a * b)\n",
        "        elif gate_id == 10:  # NOT(B)\n",
        "            return 1 - b\n",
        "        elif gate_id == 11:  # A <= B\n",
        "            return 1 - b + a * b\n",
        "        elif gate_id == 12:  # NOT(A)\n",
        "            return 1 - a\n",
        "        elif gate_id == 13:  # A -> B\n",
        "            return 1 - a + a * b\n",
        "        elif gate_id == 14:  # NAND\n",
        "            return 1 - a * b\n",
        "        elif gate_id == 15:  # TRUE\n",
        "            return torch.ones_like(a)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i < self.num_layers:\n",
        "                new_x = []\n",
        "                for neuron_idx in range(self.hidden_dim):\n",
        "                    a = x[:, neuron_idx]\n",
        "                    b = x[:, (neuron_idx + 1) % self.hidden_dim]\n",
        "                    gate_probs = F.softmax(self.logic_weights[neuron_idx], dim=0)\n",
        "                    neuron_output = sum(\n",
        "                        gate_probs[j] * self.forward_logic_gate(a, b, j)\n",
        "                        for j in range(16)\n",
        "                    )\n",
        "                    new_x.append(neuron_output)\n",
        "                x = torch.stack(new_x, dim=1)\n",
        "            x = F.relu(x)\n",
        "        return torch.sigmoid(self.out_layer(x))"
      ],
      "metadata": {
        "id": "MCsNPUrUDDFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"monk.csv\"\n",
        "X_train, X_test, y_train, y_test = load_monk_data(filepath)\n",
        "\n",
        "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "xa91qiQUDDHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 24\n",
        "output_dim = 1\n",
        "num_layers = 6\n",
        "\n",
        "model = DifferentiableLogicGate(input_dim, hidden_dim, output_dim, num_layers)\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "qrEtEpbfDDJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train).squeeze()\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOIFOsBMDDMX",
        "outputId": "5f21a64c-9319-427d-f35a-b82c5842afd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss: 0.6429\n",
            "Epoch 20/200, Loss: 0.6422\n",
            "Epoch 30/200, Loss: 0.6423\n",
            "Epoch 40/200, Loss: 0.6421\n",
            "Epoch 50/200, Loss: 0.6421\n",
            "Epoch 60/200, Loss: 0.6261\n",
            "Epoch 70/200, Loss: 0.5467\n",
            "Epoch 80/200, Loss: 0.5103\n",
            "Epoch 90/200, Loss: 0.4089\n",
            "Epoch 100/200, Loss: 0.3473\n",
            "Epoch 110/200, Loss: 0.2690\n",
            "Epoch 120/200, Loss: 0.2102\n",
            "Epoch 130/200, Loss: 0.1288\n",
            "Epoch 140/200, Loss: 0.2194\n",
            "Epoch 150/200, Loss: 0.0761\n",
            "Epoch 160/200, Loss: 0.0198\n",
            "Epoch 170/200, Loss: 0.0017\n",
            "Epoch 180/200, Loss: 0.0001\n",
            "Epoch 190/200, Loss: 0.0000\n",
            "Epoch 200/200, Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred_test = model(X_test).squeeze().round()\n",
        "accuracy = (y_pred_test == y_test).float().mean()\n",
        "print(f\"Accuracy: {accuracy.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGp7OyPzDDOJ",
        "outputId": "80beb6af-ba49-45e7-b2c7-82176e9b4f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test_np = y_pred_test.detach().numpy()\n",
        "y_test_np = y_test.numpy()\n",
        "conf_matrix = confusion_matrix(y_test_np, y_pred_test_np)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gOPc6OkDiCE",
        "outputId": "945eb0cb-c633-4b88-8946-572d6faf9ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[77  2]\n",
            " [ 1 41]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "After training, the model achieved a high classification accuracy of **97.52%** on the test set. The confusion matrix confirms strong generalization, with only a few misclassifications. This result demonstrates that the differentiable logic gate approach is highly effective for symbolic-style learning while still maintaining strong numerical performance.\n",
        "\n",
        "This experiment confirms that logic-inspired architectures can not only learn successfully but also offer interpretable internal structures â€” bridging the gap between symbolic and neural approaches."
      ],
      "metadata": {
        "id": "fg1oc8K2RSBQ"
      }
    }
  ]
}